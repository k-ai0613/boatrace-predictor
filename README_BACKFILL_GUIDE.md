# 過去データ収集ガイド

このガイドでは、GitHub Actionsを使用して過去のレースデータを安全に収集する方法を説明します。

---

## 📋 現在のデータ状況

- **収集済み期間**: 2025年10月-11月（約2ヶ月分）
- **総レース数**: 13,660レース
- **不足データ**: 2024年以前のデータ

---

## 🎯 収集目標

機械学習モデルの精度向上のため、以下のデータを段階的に収集します：

### Phase 1: 2024年後半（優先度: 最高）
- **期間**: 2024年7月-12月（6ヶ月分）
- **推定レース数**: 約40,000レース
- **推定時間**: 各月1.5-2時間 × 6ヶ月

### Phase 2: 2024年前半
- **期間**: 2024年1月-6月（6ヶ月分）
- **推定レース数**: 約40,000レース

### Phase 3: 2023年（オプション）
- **期間**: 2023年1月-12月（1年分）
- **推定レース数**: 約80,000レース

---

## 🔧 実行方法

### ステップ1: GitHubリポジトリにアクセス

1. ブラウザで https://github.com/YOUR_USERNAME/boat を開く
2. 「Actions」タブをクリック

### ステップ2: ワークフローを選択

左サイドバーから **「Monthly Data Backfill」** を選択

### ステップ3: ワークフローを実行

1. 右上の **「Run workflow」** ボタンをクリック
2. 以下のパラメータを入力：

   | パラメータ | 値 | 説明 |
   |-----------|-----|------|
   | **year_month** | `2024-12` | 収集対象年月（YYYY-MM形式） |
   | **venues** | `24` | 全会場（デフォルトのまま） |
   | **races** | `12` | 1会場あたり12レース（デフォルトのまま） |
   | **delay** | `5.0` | リクエスト間隔5秒（デフォルトのまま） |

3. **「Run workflow」** をクリック

### ステップ4: 実行状況を確認

- ワークフローの実行状況がリアルタイムで表示されます
- 完了まで約1.5-2時間かかります
- エラーが発生した場合は、ログを確認してください

### ステップ5: 次の月を実行

前の月が完了したら、次の月を実行します：

```
2024-12 → 2024-11 → 2024-10 → ... → 2024-07
```

---

## 📅 推奨実行スケジュール

### Phase 1: 2024年後半（6ヶ月分）

| 実行日 | 対象月 | パラメータ |
|--------|--------|-----------|
| 1日目 | 2024-12 | `--year-month 2024-12` |
| 2日目 | 2024-11 | `--year-month 2024-11` |
| 3日目 | 2024-10 | `--year-month 2024-10` |
| 4日目 | 2024-09 | `--year-month 2024-09` |
| 5日目 | 2024-08 | `--year-month 2024-08` |
| 6日目 | 2024-07 | `--year-month 2024-07` |

**推定完了日**: 約1週間

---

## ⚙️ 安全性の設定

すべてのワークフローは以下の安全設定で動作します：

### レート制限
- **リクエスト間隔**: 5秒（1リクエストごとに5秒待機）
- **並行処理**: なし（1つずつ順次実行）
- **リトライ**: 最大3回（エラー時の再試行）

### 実行時間
- **タイムアウト**: 6時間（それ以上かかる場合は自動停止）
- **推奨実行時間**: 日本時間23:00-07:00（サーバー負荷が低い時間帯）

### 負荷の見積もり

1ヶ月分のデータ収集で：
- **総リクエスト数**: 約7,000-8,000リクエスト
- **実行時間**: 約1.5-2時間
- **サーバー負荷**: 非常に低い（5秒/リクエスト）

---

## 🔍 データ確認方法

### 方法1: check_data_details.pyを実行

```bash
python scraper/check_data_details.py
```

### 方法2: Supabaseダッシュボードで確認

1. https://app.supabase.com にログイン
2. 「Table Editor」→「races」テーブルを開く
3. `race_date`でフィルタして、データが追加されているか確認

---

## ❌ トラブルシューティング

### エラー: "Rate limit exceeded"

**原因**: リクエストが多すぎる
**対処法**: delayパラメータを `7.0` または `10.0` に増やして再実行

### エラー: "Timeout"

**原因**: 実行時間が6時間を超えた
**対処法**:
- venues を `12` に減らす（全会場の半分）
- 後で残りの会場を収集

### エラー: "Database connection failed"

**原因**: DATABASE_URL が設定されていない
**対処法**: GitHubリポジトリの Settings → Secrets で `DATABASE_URL` を確認

---

## 📊 完了後の次のステップ

データ収集が完了したら、機械学習モデルを再訓練します：

```bash
# データ状況確認
python scraper/check_data_details.py

# モデル訓練（1,000レース以上ある場合）
python ml/train_model.py

# または統合パイプライン実行
python ml/train_full_pipeline.py
```

---

## 💡 ヒント

1. **段階的に実行**: 一度に全期間を収集せず、月ごとに分けて実行
2. **夜間実行**: 日本時間の深夜に実行すると、サーバー負荷が低い
3. **ログを保存**: GitHub Actionsのログは90日間保存されます
4. **データベース容量**: Supabase無料枠は500MBまで（約50万レース分）

---

## 📞 サポート

問題が発生した場合は、以下を確認してください：

1. GitHub Actionsのログを確認
2. `scraper/*.txt` のログファイルを確認（アーティファクトとしてダウンロード可能）
3. Supabaseダッシュボードでデータベースの状態を確認

---

**重要**: サーバーへの負荷を最小限に抑えるため、必ずレート制限を遵守してください。
