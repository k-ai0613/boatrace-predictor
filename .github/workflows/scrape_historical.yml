name: Scrape Historical Data

on:
  schedule:
    # 毎日深夜3時に1日分だけ収集
    - cron: '0 18 * * *'  # JST 3:00 (UTC 18:00前日)
  workflow_dispatch:
    inputs:
      phase:
        description: 'Collection phase (1-5)'
        required: true
        default: '1'
      days:
        description: 'Number of days to scrape'
        required: true
        default: '1'

jobs:
  scrape-historical:
    runs-on: ubuntu-latest
    timeout-minutes: 350

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r scraper/requirements.txt

    - name: Run historical scraper
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        PHASE: ${{ github.event.inputs.phase || '1' }}
        MAX_DAYS: ${{ github.event.inputs.days || '1' }}
      run: |
        python scraper/historical_scraper.py \
          --phase $PHASE \
          --max-days $MAX_DAYS

    - name: Update progress
      run: |
        echo "Phase ${{ github.event.inputs.phase }}: Scraped on $(date)" >> progress.log
        git config user.name github-actions
        git config user.email github-actions@github.com
        git add progress.log
        git commit -m "Update historical scraping progress" || exit 0
        git push || exit 0
